{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "execution": {
     "iopub.execute_input": "2022-12-13T15:46:56.146327Z",
     "iopub.status.busy": "2022-12-13T15:46:56.145860Z",
     "iopub.status.idle": "2022-12-13T15:46:56.165596Z",
     "shell.execute_reply": "2022-12-13T15:46:56.164282Z",
     "shell.execute_reply.started": "2022-12-13T15:46:56.146286Z"
    },
    "gather": {
     "logged": 1713270238164
    },
    "id": "VwRkZBHKi4NH",
    "outputId": "cfd1cb93-c91f-412b-da46-9bc6de7bb3e6",
    "papermill": {
     "duration": 1.634616,
     "end_time": "2022-11-30T15:12:08.283686",
     "exception": false,
     "start_time": "2022-11-30T15:12:06.649070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob storage config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1713270238397
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "account_name = 'ormatprdstorage1'\n",
    "account_key = '9n99wAvTcTVBVoANyf8SHJ9cG/VRmA1C2umiyPbHOXb8Bhs578oKQxeK1Sl1DHCVYhTWH+cmNVpPuC1+7EFo8Q==' #Renew in the end of 2024\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_name = 'azureml-blobstore-5930a3cf-9d2a-4c61-8e7c-3e55e29484ec'\n",
    "blob_storage_path = 'ML_Reasults'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Specific Plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1713270238685
    }
   },
   "outputs": [],
   "source": [
    "class SelectPlantTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self,plant):\n",
    "        self.plant=plant\n",
    "\n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    "\n",
    "\n",
    "    def select_plant(self, df):\n",
    "        try:\n",
    "            #df.drop(df[df['Plant']!=self.plant].index, inplace = True)\n",
    "            df=df[df['Plant']==self.plant]\n",
    "\n",
    "            #print('Select plant: ' + self.plant)\n",
    "         \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.select_plant(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Tungsten Power values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1713270239079
    }
   },
   "outputs": [],
   "source": [
    "class FixTungstenTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    "\n",
    "\n",
    "    def fix_values(self, df):\n",
    "        try:\n",
    "            filtered_df = df[df['DateTime'] >= '2023-09-20 07:00:00']\n",
    "            filtered_df['Net_Power'] *= 1.5\n",
    "            filtered_df['Power'] = np.where(filtered_df['Solar'].notna(), filtered_df['Net_Power'] - filtered_df['Solar'], filtered_df['Net_Power'])\n",
    "\n",
    "            df.update(filtered_df)\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.fix_values(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add previous n days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1713270239608
    }
   },
   "outputs": [],
   "source": [
    "class PreviousDaysTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self, num_of_days=5):\n",
    "        self.num_of_days = num_of_days\n",
    "\n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def add_days(self, df):\n",
    "        \n",
    "        def previous_day(df, i):\n",
    "            previous = pd.merge(df, df, left_on=f'prev_{i}', right_on=df.index, right_index=True, how='left')\n",
    "            previous.columns = previous.columns.str.rstrip('_x')\n",
    "            previous = previous.rename(columns={'Power_y':f'Net_Power_{i}', 'Temp_y':f'Temp_{i}'})\n",
    "            previous = previous.drop([col for col in previous.columns if '_y' in col],axis=1)\n",
    "            return previous\n",
    "        \n",
    "        try:\n",
    "            df = df.set_index('DateTime')\n",
    "            for i in range(1,self.num_of_days+1):\n",
    "                df[f'prev_{i}'] = df.index - pd.DateOffset(days=i)\n",
    "                \n",
    "            previous_days = [previous_day(df, 1)]\n",
    "            \n",
    "            for i in range(2, self.num_of_days+1):\n",
    "                previous_days.append(previous_day(previous_days[i-2], i))\n",
    "                 \n",
    "            \n",
    "            df = previous_days[-1].drop([f'prev_{i}' for i in range(1,self.num_of_days+1)],axis=1)\n",
    "\n",
    "            # back fill NaN values\n",
    "            df[[f'Net_Power_{i}' for i in range(1,self.num_of_days+1)]] = df[[f'Net_Power_{i}' for i in range(1,self.num_of_days+1)]].bfill().ffill()\n",
    "            df[[f'Temp_{i}' for i in range(1,self.num_of_days+1)]] = df[[f'Temp_{i}' for i in range(1,self.num_of_days+1)]].bfill().ffill()\n",
    "            \n",
    "            df = df.reset_index()\n",
    "            del previous_days\n",
    "            # print(f'previous {self.num_of_days} days added successfully')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.add_days(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get last x days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1713270239964
    }
   },
   "outputs": [],
   "source": [
    "class LastXDaysTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self,num_of_days=''):\n",
    "        self.num_of_days=num_of_days\n",
    " \n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    " \n",
    " \n",
    "    def get_last_x_days(self, df):\n",
    "        try:\n",
    "            if self.num_of_days!='':    \n",
    "                if 'DateTime' in df.columns:\n",
    "                    df = df.set_index('DateTime')\n",
    "                df = df[df.last_valid_index()-pd.DateOffset(self.num_of_days, 'D'):]\n",
    "                df = df.reset_index()\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        return df\n",
    " \n",
    "    def transform(self, X):\n",
    "        X = self.get_last_x_days(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1713270240280
    }
   },
   "outputs": [],
   "source": [
    "class DropOutliersTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self):        \n",
    "        pass\n",
    " \n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    " \n",
    " \n",
    "    def drop_outliers(self, df):\n",
    "        try:\n",
    "            df['year'] = df['DateTime'].dt.year\n",
    "            df['month'] = df['DateTime'].dt.month\n",
    "            monthly_avg = df.groupby(['year', 'month'])['Net_Power'].mean().reset_index()\n",
    "            monthly_avg = monthly_avg[['year', 'month', 'Net_Power']]\n",
    "            df = df.merge(monthly_avg, on=['year', 'month'], how='left')\n",
    "            df = df[df['Net_Power_x']>=(df['Net_Power_y']/3)]\n",
    "            #print('dropped successfully')\n",
    "\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        return df\n",
    " \n",
    "    def transform(self, X):\n",
    "        X = self.drop_outliers(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWcbK0f2i4Nh",
    "papermill": {
     "duration": 0.010378,
     "end_time": "2022-11-30T15:12:09.588912",
     "exception": false,
     "start_time": "2022-11-30T15:12:09.578534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clean Data\n",
    "**Clean data where power >90 or <-90 or ==0  or null values**\n",
    "**temp >500 or fare_amount <0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T15:46:56.896075Z",
     "iopub.status.busy": "2022-12-13T15:46:56.895712Z",
     "iopub.status.idle": "2022-12-13T15:46:56.914401Z",
     "shell.execute_reply": "2022-12-13T15:46:56.912835Z",
     "shell.execute_reply.started": "2022-12-13T15:46:56.896028Z"
    },
    "gather": {
     "logged": 1713270240570
    },
    "id": "W-8O7pkBi4Ni"
   },
   "outputs": [],
   "source": [
    "class CleaningTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    "\n",
    "\n",
    "    def clean_data(self, df):\n",
    "        try:\n",
    "            df.drop(df[df['Temp']<=0].index, inplace = True)\n",
    "            df.drop(df[df['Temp']>200].index, inplace = True)\n",
    "            df.drop(df[df['Power']<=1].index, inplace = True)\n",
    "            df.drop(df[df['Power']>500].index, inplace = True)\n",
    "            #df = df.drop(['DateTime','Value'], axis = 1)\n",
    "            df.dropna(subset = ['Power'], inplace=True)\n",
    "            df.dropna(subset = ['Temp'], inplace=True)\n",
    "\n",
    "            #print('Cleaned successfully')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.clean_data(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGJqWPb4i4Ni",
    "papermill": {
     "duration": 0.010658,
     "end_time": "2022-11-30T15:12:10.001790",
     "exception": false,
     "start_time": "2022-11-30T15:12:09.991132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # Add date attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T15:46:56.916572Z",
     "iopub.status.busy": "2022-12-13T15:46:56.916139Z",
     "iopub.status.idle": "2022-12-13T15:46:56.934701Z",
     "shell.execute_reply": "2022-12-13T15:46:56.933501Z",
     "shell.execute_reply.started": "2022-12-13T15:46:56.916537Z"
    },
    "gather": {
     "logged": 1713270240874
    },
    "id": "Ck-uMTbRi4Nj"
   },
   "outputs": [],
   "source": [
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def add_date_attributes(self, df):\n",
    "        try:\n",
    "            df['DateTime']=pd.to_datetime(df['DateTime'], format='%d/%m/%Y %H:%M')\n",
    "            df['year'] = pd.DatetimeIndex(df['DateTime']).year\n",
    "            df['month'] = pd.DatetimeIndex(df['DateTime']).month\n",
    "            df['day'] = pd.DatetimeIndex(df['DateTime']).day\n",
    "            df['hour'] = pd.DatetimeIndex(df['DateTime']).hour\n",
    "            df['weekday'] = pd.DatetimeIndex(df['DateTime']).weekday\n",
    "            df['day_name'] = pd.DatetimeIndex(df['DateTime']).day_name()\n",
    "\n",
    "            df['day_part'] = pd.cut(df['hour'],[-1,6,16,25] , labels=['2','1', '3'] )## \n",
    "            df['day_part']=df['day_part'].str.replace('3', '2')\n",
    "            df['season']=pd.cut(df['month'], [0 ,2,5,9 ,11,12] , labels=['0','1','2','3','4'] )\n",
    "            df['season']=df['season'].str.replace('0', '4')\n",
    "            df['weekend']=pd.cut(df['weekday'], [-1,4,6] , labels=['1','2'] )\n",
    "\n",
    "           # df['retio']= df['Power']/df['Temp']\n",
    "           # df['std']=df['retio'].std()\n",
    "\n",
    "            #print('Date attributes add successfully')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.add_date_attributes(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTzJMZwLi4Nk"
   },
   "source": [
    "# Dummy Replace values to dummies (get_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T15:52:33.547217Z",
     "iopub.status.busy": "2022-12-13T15:52:33.546781Z",
     "iopub.status.idle": "2022-12-13T15:52:33.556296Z",
     "shell.execute_reply": "2022-12-13T15:52:33.554981Z",
     "shell.execute_reply.started": "2022-12-13T15:52:33.547181Z"
    },
    "gather": {
     "logged": 1713270241313
    },
    "id": "c-k2Z4QEi4Nl"
   },
   "outputs": [],
   "source": [
    "class DummyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "         return self\n",
    "\n",
    "    def dummy(self, df):\n",
    "        df=df[['Power','Temp','hour','weekday','day_part','season','weekend', 'Net_Power_1', 'Temp_1', 'Net_Power_2', 'Temp_2', 'Net_Power_3', 'Temp_3', 'Net_Power_4', 'Temp_4', 'Net_Power_5', 'Temp_5']]\n",
    "        df=pd.get_dummies(df,columns = ['hour','weekday','day_part','season','weekend'])\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.dummy(X)\n",
    "        #print('Get Dummies successfully')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data arrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1713270241745
    }
   },
   "outputs": [],
   "source": [
    "class WeatherDataArrangeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def data_arrangement(self, df):\n",
    "        try:\n",
    "            df = df[df['Hour'] != 25]\n",
    "            df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')\n",
    "            df['DateTime'] = df['Date'] + pd.to_timedelta(df['Hour'].astype(str) + ':00:00')\n",
    "            df = df[['DateTime', 'Location', 'Value']].rename(columns={'Value':'Temp', 'Location':'Plant'})\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.data_arrangement(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1713270242182
    }
   },
   "outputs": [],
   "source": [
    "class DaysForPredTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,last_date):\n",
    "        self.last_date=last_date\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def days_for_pred(self, df):\n",
    "        try:\n",
    "            # interpolation fill empty values\n",
    "            df['Temp'] = df['Temp'].interpolate(method='linear', limit_direction='forward')            \n",
    "            # Take 3 next days data for prediction\n",
    "            target_date_range = self.last_date + pd.DateOffset(days=3)\n",
    "            df = df[(df['DateTime'] > last_date) & (df['DateTime'] <= target_date_range)]\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.days_for_pred(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wf2aYgvi4NY"
   },
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2022-12-13T15:46:56.168908Z",
     "iopub.status.busy": "2022-12-13T15:46:56.167777Z",
     "iopub.status.idle": "2022-12-13T15:46:56.718613Z",
     "shell.execute_reply": "2022-12-13T15:46:56.717272Z",
     "shell.execute_reply.started": "2022-12-13T15:46:56.168841Z"
    },
    "gather": {
     "logged": 1713270242630
    },
    "id": "uugRHyCJi4Nb",
    "outputId": "408f10f4-a05e-4187-82f5-b06ae93ead50",
    "papermill": {
     "duration": 0.967463,
     "end_time": "2022-11-30T15:12:09.261461",
     "exception": false,
     "start_time": "2022-11-30T15:12:08.293998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DAC1 = pd.read_csv('https://ormatprdstorage1.blob.core.windows.net/azureml-blobstore-5930a3cf-9d2a-4c61-8e7c-3e55e29484ec/Azureml_Generation/Generation.csv?sp=rcw&st=2023-11-28T07:46:25Z&se=2024-12-31T15:46:25Z&spr=https&sv=2022-11-02&sr=c&sig=ZrQv6iSFtEFeKuZs8S1WhGBoQPRjmH7SJI%2BgeRxIu5Q%3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1713270243078
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Plant</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>month</th>\n",
       "      <th>Solar</th>\n",
       "      <th>Net_Power</th>\n",
       "      <th>Power</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brady</td>\n",
       "      <td>2023-07-01 00:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brady</td>\n",
       "      <td>2023-07-01 01:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brady</td>\n",
       "      <td>2023-07-01 02:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brady</td>\n",
       "      <td>2023-07-01 03:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brady</td>\n",
       "      <td>2023-07-01 04:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Plant             DateTime  month  Solar  Net_Power  Power  Temp\n",
       "0  Brady  2023-07-01 00:00:00      7    NaN        NaN    NaN   NaN\n",
       "1  Brady  2023-07-01 01:00:00      7    NaN        NaN    NaN   NaN\n",
       "2  Brady  2023-07-01 02:00:00      7    NaN        NaN    NaN   NaN\n",
       "3  Brady  2023-07-01 03:00:00      7    NaN        NaN    NaN   NaN\n",
       "4  Brady  2023-07-01 04:00:00      7    NaN        NaN    NaN   NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DAC1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6R-IUwONi4Nc",
    "papermill": {
     "duration": 0.011608,
     "end_time": "2022-11-30T15:12:09.283396",
     "exception": false,
     "start_time": "2022-11-30T15:12:09.271788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Date Column to DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-13T15:46:56.720266Z",
     "iopub.status.busy": "2022-12-13T15:46:56.719902Z",
     "iopub.status.idle": "2022-12-13T15:46:56.759117Z",
     "shell.execute_reply": "2022-12-13T15:46:56.757838Z",
     "shell.execute_reply.started": "2022-12-13T15:46:56.720232Z"
    },
    "gather": {
     "logged": 1713270243882
    },
    "id": "4pl-kNO3i4Nc",
    "outputId": "13391152-1b62-48c1-c196-487436fb32e5",
    "papermill": {
     "duration": 0.061981,
     "end_time": "2022-11-30T15:12:09.355495",
     "exception": false,
     "start_time": "2022-11-30T15:12:09.293514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DAC1['DateTime']= pd.to_datetime(DAC1['DateTime'], format='%d-%m-%Y %H:%M:%S')\n",
    "DAC1['DateTime']= pd.to_datetime(DAC1['DateTime'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1713270244389
    }
   },
   "outputs": [],
   "source": [
    "weather_forecast_data = pd.read_csv('https://ormatprdstorage1.blob.core.windows.net/azureml-blobstore-5930a3cf-9d2a-4c61-8e7c-3e55e29484ec/Azureml_Generation/Forecast.csv?sp=rcw&st=2023-11-28T07:46:25Z&se=2024-12-31T15:46:25Z&spr=https&sv=2022-11-02&sr=c&sig=ZrQv6iSFtEFeKuZs8S1WhGBoQPRjmH7SJI%2BgeRxIu5Q%3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1713270298112
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F480_Plants_Generation_HHKey</th>\n",
       "      <th>F480_CK</th>\n",
       "      <th>HourlyDataID</th>\n",
       "      <th>Hour</th>\n",
       "      <th>D61_Sub_Site_Dim_Key</th>\n",
       "      <th>Value</th>\n",
       "      <th>Participant</th>\n",
       "      <th>HourlyData_Type</th>\n",
       "      <th>HourlyDataTypeDesc</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Source</th>\n",
       "      <th>OP_Inser_Date</th>\n",
       "      <th>OP_Update_Date</th>\n",
       "      <th>OP_User_IP</th>\n",
       "      <th>OP_User_Name</th>\n",
       "      <th>OP_Source_Process</th>\n",
       "      <th>OP_insert_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13645</th>\n",
       "      <td>20100783</td>\n",
       "      <td>HourlyDataView-623258-25</td>\n",
       "      <td>623258</td>\n",
       "      <td>25</td>\n",
       "      <td>177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CS</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Weather Forecast Temp</td>\n",
       "      <td>CS9</td>\n",
       "      <td>2024-04-25 00:00:00.0</td>\n",
       "      <td>HourlyDataView</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-04-2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Talend_BI</td>\n",
       "      <td>Master_F480_Plant_Generation_HH</td>\n",
       "      <td>15-04-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13646</th>\n",
       "      <td>20098450</td>\n",
       "      <td>HourlyDataView-623146-25</td>\n",
       "      <td>623146</td>\n",
       "      <td>25</td>\n",
       "      <td>233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RAFT RIVER</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Weather Forecast Temp</td>\n",
       "      <td>RAFT RIVER</td>\n",
       "      <td>2024-04-25 00:00:00.0</td>\n",
       "      <td>HourlyDataView</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-04-2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Talend_BI</td>\n",
       "      <td>Master_F480_Plant_Generation_HH</td>\n",
       "      <td>15-04-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647</th>\n",
       "      <td>20101955</td>\n",
       "      <td>HourlyDataView-623223-25</td>\n",
       "      <td>623223</td>\n",
       "      <td>25</td>\n",
       "      <td>223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JERSEY VALLEY</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Weather Forecast Temp</td>\n",
       "      <td>JERSEY VALLEY</td>\n",
       "      <td>2024-04-25 00:00:00.0</td>\n",
       "      <td>HourlyDataView</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-04-2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Talend_BI</td>\n",
       "      <td>Master_F480_Plant_Generation_HH</td>\n",
       "      <td>15-04-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13648</th>\n",
       "      <td>20102090</td>\n",
       "      <td>HourlyDataView-623239-25</td>\n",
       "      <td>623239</td>\n",
       "      <td>25</td>\n",
       "      <td>-99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SBHR</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Weather Forecast Temp</td>\n",
       "      <td>SBHills</td>\n",
       "      <td>2024-04-25 00:00:00.0</td>\n",
       "      <td>HourlyDataView</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-04-2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Talend_BI</td>\n",
       "      <td>Master_F480_Plant_Generation_HH</td>\n",
       "      <td>15-04-2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13649</th>\n",
       "      <td>20101119</td>\n",
       "      <td>HourlyDataView-623250-25</td>\n",
       "      <td>623250</td>\n",
       "      <td>25</td>\n",
       "      <td>234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAN EMIDIO</td>\n",
       "      <td>Temperature</td>\n",
       "      <td>Weather Forecast Temp</td>\n",
       "      <td>SAN EMIDIO</td>\n",
       "      <td>2024-04-25 00:00:00.0</td>\n",
       "      <td>HourlyDataView</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-04-2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Talend_BI</td>\n",
       "      <td>Master_F480_Plant_Generation_HH</td>\n",
       "      <td>15-04-2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F480_Plants_Generation_HHKey                   F480_CK  HourlyDataID  \\\n",
       "13645                      20100783  HourlyDataView-623258-25        623258   \n",
       "13646                      20098450  HourlyDataView-623146-25        623146   \n",
       "13647                      20101955  HourlyDataView-623223-25        623223   \n",
       "13648                      20102090  HourlyDataView-623239-25        623239   \n",
       "13649                      20101119  HourlyDataView-623250-25        623250   \n",
       "\n",
       "       Hour  D61_Sub_Site_Dim_Key  Value    Participant HourlyData_Type  \\\n",
       "13645    25                   177    NaN             CS     Temperature   \n",
       "13646    25                   233    NaN     RAFT RIVER     Temperature   \n",
       "13647    25                   223    NaN  JERSEY VALLEY     Temperature   \n",
       "13648    25                   -99    NaN           SBHR     Temperature   \n",
       "13649    25                   234    NaN     SAN EMIDIO     Temperature   \n",
       "\n",
       "          HourlyDataTypeDesc       Location                   Date  \\\n",
       "13645  Weather Forecast Temp            CS9  2024-04-25 00:00:00.0   \n",
       "13646  Weather Forecast Temp     RAFT RIVER  2024-04-25 00:00:00.0   \n",
       "13647  Weather Forecast Temp  JERSEY VALLEY  2024-04-25 00:00:00.0   \n",
       "13648  Weather Forecast Temp        SBHills  2024-04-25 00:00:00.0   \n",
       "13649  Weather Forecast Temp     SAN EMIDIO  2024-04-25 00:00:00.0   \n",
       "\n",
       "               Source  OP_Inser_Date OP_Update_Date  OP_User_IP OP_User_Name  \\\n",
       "13645  HourlyDataView            NaN     16-04-2024         NaN    Talend_BI   \n",
       "13646  HourlyDataView            NaN     16-04-2024         NaN    Talend_BI   \n",
       "13647  HourlyDataView            NaN     16-04-2024         NaN    Talend_BI   \n",
       "13648  HourlyDataView            NaN     16-04-2024         NaN    Talend_BI   \n",
       "13649  HourlyDataView            NaN     16-04-2024         NaN    Talend_BI   \n",
       "\n",
       "                     OP_Source_Process OP_insert_Date  \n",
       "13645  Master_F480_Plant_Generation_HH     15-04-2024  \n",
       "13646  Master_F480_Plant_Generation_HH     15-04-2024  \n",
       "13647  Master_F480_Plant_Generation_HH     15-04-2024  \n",
       "13648  Master_F480_Plant_Generation_HH     15-04-2024  \n",
       "13649  Master_F480_Plant_Generation_HH     15-04-2024  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_forecast_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plants list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1713270244905
    }
   },
   "outputs": [],
   "source": [
    "plant_dict = {'Brady':'BRADY',\n",
    " 'Don Campbell 1':'DAC',\n",
    " 'Don Campbell 2':'DAC',\n",
    " 'Galena2':'GALENA 2',\n",
    " 'MGH3':'MGH1',\n",
    " 'SBHR':'SBHills',\n",
    " 'SB2-3':'SB2',\n",
    " 'Tungsten':'TUNGSTEN'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1713270245558
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time data \"2024-04-12 00:00:00.0\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "'Plant'\n",
      "'Temp'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m X_test\u001b[38;5;241m=\u001b[39mtest_data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPower\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Linear Regression model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m LinearReg \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Loop over 3 days and predict\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Calculate the start and end indices for the current loop\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/linear_model/_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    644\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    646\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 648\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[1;32m    653\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    654\u001b[0m )\n\u001b[1;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[1;32m    657\u001b[0m     X,\n\u001b[1;32m    658\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    662\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/utils/validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[0;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "for plant in plant_dict.keys():\n",
    "    # Select plant\n",
    "    generation = SelectPlantTransformer(plant).transform(DAC1)\n",
    "    if plant == 'Tungsten':\n",
    "        generation = FixTungstenTransformer().transform(generation)\n",
    "    # Add previous 5 days \n",
    "    generation = PreviousDaysTransformer().transform(generation)\n",
    "    # get x last days\n",
    "    generation = LastXDaysTransformer(45).transform(generation)\n",
    "    # Drop Outliers\n",
    "    generation = DropOutliersTransformer().transform(generation)\n",
    "    # Clean\n",
    "    generation = CleaningTransformer().transform(generation)\n",
    "    \n",
    "    generation = generation.sort_values(['DateTime']).reindex()\n",
    "    # Get last date for prediction\n",
    "    last_date = generation['DateTime'].max()\n",
    "    # Weather forcast data arrangement\n",
    "    weather_forecast =  WeatherDataArrangeTransformer().transform(weather_forecast_data)    \n",
    "    # Select plant in weather data\n",
    "    weather_forecast = SelectPlantTransformer(plant_dict[plant]).transform(weather_forecast)\n",
    "    # Prepare data for prediction\n",
    "    weather_forecast = DaysForPredTransformer(last_date).transform(weather_forecast)\n",
    "   \n",
    "    # Combine original data with prediction data\n",
    "    all_data = pd.concat([generation, weather_forecast], ignore_index=True)\n",
    "    \n",
    "    if all_data.empty:\n",
    "        predictions = all_data[['DateTime','Plant','Temp','Power']]\n",
    "    else:\n",
    "        # Date Arrangement\n",
    "        all_data = DateTransformer().transform(all_data)\n",
    "        # Save date column for output \n",
    "        dates = all_data['DateTime']\n",
    "        # Convert 00 to 24\n",
    "        dates = dates.apply(lambda x: x - pd.DateOffset(days=1) if x.hour == 0 else x)\n",
    "        dates = dates.dt.strftime('%Y-%m-%d %H:00:00').str.replace(' 00:', ' 24:')\n",
    "        \n",
    "        # Add Dummy\n",
    "        all_data = DummyTransformer().transform(all_data)\n",
    "\n",
    "        train_data = all_data.iloc[:-72]\n",
    "        test_data = all_data.iloc[-72:]\n",
    "\n",
    "        # Train test arrangement\n",
    "        y_train=train_data['Power'] \n",
    "        X_train=train_data.drop('Power', axis=1)\n",
    "\n",
    "        y_test=test_data['Power'] \n",
    "        X_test=test_data.drop('Power', axis=1)\n",
    "\n",
    "        # Linear Regression model\n",
    "        LinearReg = LinearRegression().fit(X_train, y_train)\n",
    "        # Loop over 3 days and predict\n",
    "        for i in range(3):\n",
    "            # Calculate the start and end indices for the current loop\n",
    "            start_idx = i * 24\n",
    "            end_idx = (i + 1) * 24  \n",
    "            # Get the data for the current loop\n",
    "            current_day = test_data.iloc[start_idx:end_idx]   \n",
    "            previous_day = all_data.iloc[current_day.index - 24]\n",
    "            previous_day = previous_day.reset_index(drop=True)\n",
    "            ind = current_day.index.values\n",
    "            current_day = current_day.reset_index(drop=True)\n",
    "            current_day[current_day.columns[2:12]] = previous_day[previous_day.columns[:10]]\n",
    "            current_day['Power']  = LinearReg.predict(current_day.drop('Power', axis=1))\n",
    "            current_day = current_day.set_index(ind)\n",
    "            all_data.iloc[ind] = current_day\n",
    "        # Organize predictions columns\n",
    "        all_data_copy = all_data.copy()\n",
    "        all_data_copy['DateTime'] = dates\n",
    "        \n",
    "        all_data_copy['Plant'] = plant\n",
    "        predictions = all_data_copy[['DateTime','Plant','Temp','Power']].tail(72).reset_index(drop=True)\n",
    "    \n",
    "    # save predictions to csv \n",
    "    csv_data = predictions.to_csv(index=False)\n",
    "    # Specify the blob name and upload the CSV data to the container:\n",
    "    blob_name = f\"{blob_storage_path}/LR_results_{plant}.csv\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    blob_client.upload_blob(csv_data, overwrite=True)\n",
    "    print(\"saved successfuly!\")\n",
    "    \n",
    "    if not all_data.empty:\n",
    "        # KNN model\n",
    "        Neigh = KNeighborsRegressor(n_neighbors=5).fit(X_train, y_train)\n",
    "        # Loop over 3 days and predict\n",
    "        for i in range(3):\n",
    "            # Calculate the start and end indices for the current loop\n",
    "            start_idx = i * 24\n",
    "            end_idx = (i + 1) * 24  \n",
    "            # Get the data for the current loop\n",
    "            current_day = test_data.iloc[start_idx:end_idx]   \n",
    "            previous_day = all_data.iloc[current_day.index - 24]\n",
    "            previous_day = previous_day.reset_index(drop=True)\n",
    "            ind = current_day.index.values\n",
    "            current_day = current_day.reset_index(drop=True)\n",
    "            current_day[current_day.columns[2:12]] = previous_day[previous_day.columns[:10]]\n",
    "            current_day['Power']  = Neigh.predict(current_day.drop('Power', axis=1))\n",
    "            current_day = current_day.set_index(ind)\n",
    "            all_data.iloc[ind] = current_day\n",
    "\n",
    "        # Organize predictions columns\n",
    "        all_data['DateTime'] = dates\n",
    "        all_data['Plant'] = plant\n",
    "        predictions = all_data[['DateTime','Plant','Temp','Power']].tail(72).reset_index(drop=True)\n",
    "\n",
    "    # save predictions to csv \n",
    "    csv_data = predictions.to_csv(index=False)\n",
    "    # Specify the blob name and upload the CSV data to the container:\n",
    "    blob_name = f\"{blob_storage_path}/KNN5_results_{plant}.csv\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    blob_client.upload_blob(csv_data, overwrite=True)\n",
    "    print(\"saved successfuly!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
