{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install neuralprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1707051768351
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from neuralprophet import NeuralProphet\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger('neuralprophet').disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob storage config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "account_name = 'ormatprdstorage1'\n",
    "account_key = '9n99wAvTcTVBVoANyf8SHJ9cG/VRmA1C2umiyPbHOXb8Bhs578oKQxeK1Sl1DHCVYhTWH+cmNVpPuC1+7EFo8Q==' #Renew in the end of 2024\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={account_key};EndpointSuffix=core.windows.net\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_name = 'azureml-blobstore-5930a3cf-9d2a-4c61-8e7c-3e55e29484ec'\n",
    "blob_storage_path = 'ML_Reasults'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Specific Plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1701856913142
    }
   },
   "outputs": [],
   "source": [
    "class SelectPlantTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,plant):\n",
    "        self.plant=plant\n",
    "\n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    "\n",
    "    def select_plant(self, df):\n",
    "        try:\n",
    "            df=df[df['Location']==self.plant]\n",
    "         \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.select_plant(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparationTransformer(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y')\n",
    "            df['date'] = pd.to_datetime(df['date'].dt.date)\n",
    "            df['DateTime'] = df['date'] + pd.to_timedelta(df['hour'].astype(str) + ':00:00')\n",
    "\n",
    "            df = df.rename(columns = {\"DateTime\": \"ds\",\"Value\": \"y\"})\n",
    "\n",
    "            df = df[[\"ds\",\"y\"]]\n",
    "\n",
    "            df = df.dropna()\n",
    " \n",
    "            df = df.set_index('ds')\n",
    "            df = df.sort_index()\n",
    "            df = df[~df.index.duplicated(keep='first')]\n",
    "            df = df.asfreq('h')\n",
    "            df['y'] = df['y'].bfill().ffill()\n",
    "            df = df.reset_index()\n",
    "            df = df.sort_index()\n",
    "        \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.prepare_data(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get last x years of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryYearsTransformer(BaseEstimator, TransformerMixin):\n",
    "   #######\n",
    "    def __init__(self,years='',months=0):\n",
    "        self.years = years\n",
    "        self.months = months\n",
    " \n",
    "    def fit(self, X):  #\n",
    "        return self\n",
    " \n",
    " \n",
    "    def get_last_x_years(self, df):\n",
    "        try:\n",
    "            if self.years!='':    \n",
    "                latest_date = df['ds'].max()\n",
    "                start_date = latest_date - pd.DateOffset(years=self.years,months=self.months)           \n",
    "                df = df[df['ds'] >= start_date]\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "        return df\n",
    " \n",
    "    def transform(self, X):\n",
    "        X = self.get_last_x_years(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number Of Hours To Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoursToPredictTransformer(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    def __init__(self, pred_hours=''):\n",
    "        self.pred_hours = pred_hours\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def split_data(self, df):\n",
    "        try:\n",
    "            if self.pred_hours != '':\n",
    "                df = df[:-int(self.pred_hours)]\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.split_data(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data Per Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoursTransformer(BaseEstimator, TransformerMixin):\n",
    "   \n",
    "    def __init__(self, i=0):\n",
    "        self.i = i\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def split_data(self, df):\n",
    "        try:\n",
    "            if self.i!=0:\n",
    "                df['hour'] = df['ds'].dt.hour\n",
    "                df = df[df['hour']==self.i][['ds','y']]\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = self.split_data(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "z1M9wkamWDFR"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://ormatprdstorage1.blob.core.windows.net/azureml-blobstore-5930a3cf-9d2a-4c61-8e7c-3e55e29484ec/Azureml_Generation/SpotPrice.csv?sp=rcw&st=2023-11-28T07:46:25Z&se=2024-12-31T15:46:25Z&spr=https&sv=2022-11-02&sr=c&sig=ZrQv6iSFtEFeKuZs8S1WhGBoQPRjmH7SJI%2BgeRxIu5Q%3D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plants List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plants_dict = {'BRADY':'Brady',\n",
    "               'DAC 1':'Don Campbell 1',\n",
    "               'DAC 2':'Don Campbell 2', \n",
    "               'TUNGSTEN':'Tungsten',  \n",
    "               'GALENA 2':'Galena2', \n",
    "               'McGINNESS 3':'MGH3',\n",
    "               'STEAMBOAT':'SBHR', \n",
    "               'STEAMBOAT 2-3':'SB2-3'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions for next 38 hours and save to blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.995% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n",
      "Missing logger folder: /mnt/batch/tasks/shared/LS_root/mounts/clusters/dkazakov2/code/Users/AzureBI/Predictions/Spot_Price/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   5%|▌         | 5/94 [00:00<00:00, 2198.04it/s, loss=0.0197, v_num=1, MAE=314.0, RMSE=395.0, Loss=0.0325, RegLoss=0.000].000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.996% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.624% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.624% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Defined frequency is equal to major frequency - H\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.24it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - (NP.df_utils.return_df_in_original_format) - Returning df with no ID column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - (NP.forecaster.fit) - When Global modeling with local normalization, metrics are displayed in normalized scale.\n",
      "INFO - (NP.df_utils._infer_frequency) - Major frequency H corresponds to 99.996% of the data.\n",
      "INFO - (NP.df_utils._infer_frequency) - Dataframe freq automatically defined as H\n",
      "INFO - (NP.config.init_data_params) - Setting normalization to global as only one dataframe provided for training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   5%|▌         | 5/94 [00:13<03:58,  2.68s/it, loss=0.0197, v_num=1, MAE=314.0, RMSE=395.0, Loss=0.0325, RegLoss=0.000]  \n",
      "Epoch 11:  12%|█▏        | 11/94 [00:00<00:00, 4016.13it/s, loss=0.000984, v_num=2, MAE=46.50, RMSE=74.10, Loss=0.00103, RegLoss=0.000]"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Set the logging level to WARNING or ERROR to reduce output\n",
    "logging.getLogger(\"lightning\").setLevel(logging.WARNING)  # Suppress INFO logs\n",
    "logging.getLogger(\"NP\").setLevel(logging.WARNING)  # Suppress Neural Prophet logs\n",
    "\n",
    "for plant in plants_dict.keys():\n",
    "    \n",
    "    spot_price = SelectPlantTransformer(plant).transform(df)\n",
    "    spot_price = DataPreparationTransformer().transform(spot_price)\n",
    "    spot_price = HistoryYearsTransformer(years=1,months=0).transform(spot_price)\n",
    "\n",
    "    model = NeuralProphet(\n",
    "    yearly_seasonality=False,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=True,\n",
    "    n_lags= 6*38,\n",
    "    ar_layers= [32,32,32],\n",
    "    learning_rate=0.005,\n",
    "    n_forecasts=38,\n",
    "    epochs=94,\n",
    "    batch_size=64\n",
    "    )\n",
    "    \n",
    "    model.fit(spot_price)\n",
    "    \n",
    "    \n",
    "    future = model.make_future_dataframe(spot_price, periods=38)\n",
    "    forecast = model.predict(future)\n",
    "    forecast = model.get_latest_forecast(forecast)\n",
    "    \n",
    "    dates = forecast['ds'].values\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'DateTime': dates,\n",
    "        'Plant': plants_dict[plant],\n",
    "        'Temp': 'null',\n",
    "        'Spot_price': forecast['origin-0'].values\n",
    "    })\n",
    "\n",
    "    # Convert 00 to 24\n",
    "    results['DateTime'] = results['DateTime'].apply(lambda x: x - pd.DateOffset(days=1) if x.hour == 0 else x)\n",
    "    results['DateTime'] = results['DateTime'].dt.strftime('%Y-%m-%d %H:00:00').str.replace(' 00:', ' 24:')\n",
    "    \n",
    "    csv_data = results.to_csv(index=False)\n",
    "    # Specify the blob name and upload the CSV data to the container:\n",
    "    blob_name = f\"{blob_storage_path}/spot_price_{plants_dict[plant]}.csv\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    blob_client.upload_blob(csv_data, overwrite=True)\n",
    "\n",
    "    # Clean lighting_logs directory to reduce memory\n",
    "\n",
    "    def cleanup_logs(log_dir):\n",
    "        if os.path.exists(log_dir):\n",
    "            shutil.rmtree(log_dir)\n",
    "\n",
    "    cleanup_logs('./lightning_logs/')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
